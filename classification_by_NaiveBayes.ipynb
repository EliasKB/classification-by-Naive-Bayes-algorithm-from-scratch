{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as npzz\n",
    "from collections import Counter\n",
    "from codecs import open\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_documents(doc_file): \n",
    "    docs = []\n",
    "    labels = []\n",
    "    with open(doc_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            docs.append(words[3:])\n",
    "            labels.append(words[1])\n",
    "    return docs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naiveBayes(documents, labels):\n",
    "    \n",
    "    freq_train_labels = Counter()\n",
    "    for doc in labels:\n",
    "        freq_train_labels[doc] += 1\n",
    "        \n",
    "        \n",
    "        \n",
    "    \"\"\"  P(a doc is annotated as positive or negative )  \"\"\"\n",
    "\n",
    "    ProbabilityOfPositive = freq_train_labels['pos']/len(documents) \n",
    "    ProbabilityOfNegative = freq_train_labels['neg']/len(documents) \n",
    "    print('Probability that a review in a corpus is annotated as positive is: ', ProbabilityOfPositive)\n",
    "    print('Probability that a review in a corpus is annotated as negative is: ', ProbabilityOfNegative)\n",
    " \n",
    "\n",
    "\n",
    "    \"\"\"  making a list of positive and one of negative annotated review  \"\"\"\n",
    "\n",
    "    listOfposDocsInTrainingData = []\n",
    "    listOfnegDocsInTrainingData = []\n",
    "    for doc, label in zip(documents, labels):\n",
    "        if label == 'pos':\n",
    "            listOfposDocsInTrainingData.append(doc)\n",
    "        else:\n",
    "            listOfnegDocsInTrainingData.append(doc)\n",
    "            \n",
    "        \n",
    "    \"\"\"  finding the freuency of words in training data for pos resp. neg docs  \"\"\"\n",
    "    \n",
    "    freqsOfWordsInPosDocs = Counter(w for doc in listOfposDocsInTrainingData for w in doc)\n",
    "    freqsOfWordsInNegDocs = Counter(w for doc in listOfnegDocsInTrainingData for w in doc)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" Prob(a specific word in traindata is present in pos or neg annotated docs) \"\"\"\n",
    "    \n",
    "    totalfreq_pos = sum(list(freqsOfWordsInPosDocs.values()))\n",
    "    totalfreq_neg = sum(list(freqsOfWordsInNegDocs.values()))\n",
    "    \n",
    "    posprobs = freqsOfWordsInPosDocs\n",
    "    negprobs = freqsOfWordsInNegDocs\n",
    "\n",
    "    for i in posprobs:\n",
    "        posprobs[i] = posprobs[i] / totalfreq_pos\n",
    "        \n",
    "    for i in negprobs:\n",
    "        negprobs[i] = negprobs[i] / totalfreq_neg\n",
    "        \n",
    "    \n",
    "    return(posprobs, negprobs, ProbabilityOfPositive, ProbabilityOfNegative, totalfreq_pos, totalfreq_neg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" depending whether the prob of positive or negative is higher, annotate the high one as a predicted label \"\"\"\n",
    "def classify_label_byprobs(probOfPos, ProbOfNeg):\n",
    "    if(probOfPos > ProbOfNeg):\n",
    "        guess = 'pos'\n",
    "    else:\n",
    "        guess = 'neg'\n",
    "    return(guess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Given the predicted and true labels calclutate the ration of right-classified and missclassified labels\"\"\"\n",
    "def accuracy(true_labels, guessed_labels):\n",
    "    counter = 0\n",
    "    for i in range(len(guessed_labels)):\n",
    "        if(guessed_labels[i] == true_labels[i]):\n",
    "            counter += 1\n",
    "    return counter / len(guessed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  classify the reviews in the eval/test data as pos or negative labeled \"\"\"\n",
    "def classify_documents(docs, alpha, Posprobs, Negprobs, PriorPositive, PriorNegative, totalPos, totalNeg):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    NchoicesP = len(Posprobs) # uses for laplace smoothing\n",
    "    NchoicesN = len(Negprobs)\n",
    "\n",
    "    \"\"\"\n",
    "        for laplace-smoothing please check https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        Check whether if the word is seen or unseen and use additive smoothing\n",
    "    \"\"\"\n",
    "    \n",
    "    for doc in docs:\n",
    "            \n",
    "        probOfAReviewBeingPos = 0\n",
    "        probOfAReviewBeingNeg = 0\n",
    "        \n",
    "        for word in doc: \n",
    "         \n",
    "            \"\"\" if the word is seen e.i. existed in training data \"\"\"\n",
    "            if Posprobs[word] > 0:                                  \n",
    "                probOfAReviewBeingPos = probOfAReviewBeingPos +  np.log((Posprobs[word] * totalPos + alpha)/(alpha * NchoicesP + totalPos))\n",
    "            else:                                                \n",
    "                probOfAReviewBeingPos = probOfAReviewBeingPos + np.log(alpha / (alpha * NchoicesP + totalPos))\n",
    "                \n",
    "                \n",
    "            if Negprobs[word] > 0:\n",
    "                probOfAReviewBeingNeg = probOfAReviewBeingNeg + np.log((Negprobs[word] * totalNeg + alpha) / (alpha * NchoicesN + totalNeg))\n",
    "            else:\n",
    "                probOfAReviewBeingNeg = probOfAReviewBeingNeg + np.log(alpha / (alpha * NchoicesN + totalNeg))\n",
    "            \n",
    "            \n",
    "        \"\"\"  Posterior ∝ likelihood * prior ==> Posterior ∝ log(likelihood) + log(prior) \"\"\"  \n",
    "        posteriorPosPerDoc = probOfAReviewBeingPos + np.log(PriorPositive)\n",
    "        posteriorNegPerDoc = probOfAReviewBeingNeg + np.log(PriorNegative)\n",
    "        \n",
    "        predicted_labels.append(classify_label_byprobs(posteriorPosPerDoc, posteriorNegPerDoc)) \n",
    "        \n",
    "    return(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  Reaading the corpus \"\"\"\n",
    "all_docs, all_labels = read_documents('all_sentiment_shuffled.txt')\n",
    "\n",
    "\"\"\"  Dividing data into the train 80% and evaluation part  \"\"\"\n",
    "split_point = int(0.80*len(all_docs)) \n",
    "train_docs = all_docs[:split_point]   \n",
    "train_labels = all_labels[:split_point] \n",
    "eval_docs = all_docs[split_point:] \n",
    "eval_labels = all_labels[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that a review in a corpus is annotated as positive is:  0.5085510439618088\n",
      "Probability that a review in a corpus is annotated as negative is:  0.49144895603819116\n",
      "The accuracy of predicted labels is:  0.8124213176668066\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  send inputs to the functions and get outputs  \"\"\"\n",
    "Posprobs,Negprobs,ProbabilityOfPositive,ProbabilityOfNegative, totalPos, totalNeg = train_naiveBayes(train_docs, train_labels)\n",
    "\n",
    "alpha_laplace = 1.0\n",
    "predicted_labels1 = classify_documents(eval_docs,alpha_laplace,Posprobs,Negprobs,ProbabilityOfPositive,ProbabilityOfNegative, totalPos, totalNeg)\n",
    "Accuracyy = accuracy(eval_labels, predicted_labels1)\n",
    "\n",
    "print('The accuracy of predicted labels is: ' ,Accuracyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  The function return shortest documents with desired lenght  \"\"\"\n",
    "def findSome_ShortestDocs(docs, labels, doc_length):\n",
    "    shortdocs = []\n",
    "    corresponinglabels = []\n",
    "    k = 0\n",
    "    for doc in docs:\n",
    "        if len(doc) < doc_length:\n",
    "            shortdocs.append(doc)\n",
    "            corresponinglabels.append(labels[k])\n",
    "        k += 1\n",
    "    return(shortdocs, corresponinglabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  The function return missclassified documents and their true label  \"\"\"    \n",
    "def misclassificated_docs_And_lebels(test_docs ,true_labels, guessed_labels):\n",
    "    truelabel_for_Misc = [] \n",
    "    miss_docs_eval = []\n",
    "    \n",
    "    for i in range(len(guessed_labels)):\n",
    "        if(guessed_labels[i] != true_labels[i]):\n",
    "            miss_docs_eval.append(test_docs[i])\n",
    "            truelabel_for_Misc.append(true_labels[i])\n",
    "\n",
    "    return(miss_docs_eval, truelabel_for_Misc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'would', 'have', 'liked', 'a', 'more', 'real', 'story', 'in', 'this', 'dvd'], ['it', \"'s\", 'not', 'great', 'music', 'to', 'dance', 'to'], ['goo']]\n",
      "['neg', 'neg', 'pos']\n",
      "0.004602605646306818\n",
      "0.007199447207783851\n",
      "0.0027854225852272725\n",
      "0.001152582960795394\n",
      "1.3871626420454546e-06\n",
      "4.196297187361872e-06\n",
      "One reaason of not-higher-accuracy-than-81% is that the model dont set higher negative prob. to the word \"not\" and higher positive prob. to the word \"great\" \n",
      "Other reason can be failure in annotating the reviews and difficulty of annotating to neutral words such as \"goo\" \n"
     ]
    }
   ],
   "source": [
    "\"\"\"  The possible reasons behind the missclassifaication  \"\"\"\n",
    "missdocs, misslabels = misclassificated_docs_And_lebels (eval_docs, eval_labels, predicted_labels1)\n",
    "\n",
    "shortestmiss_docs, correpondedlabels= findSome_ShortestDocs(missdocs, misslabels, 12)\n",
    "\n",
    "print(shortestmiss_docs)\n",
    "print(correpondedlabels)\n",
    "\n",
    "print(Posprobs['not'])\n",
    "print(Negprobs['not'])\n",
    "print(Posprobs['great'])\n",
    "print(Negprobs['great'])\n",
    "print(Posprobs['goo'])\n",
    "print(Negprobs['goo'])\n",
    "print('One reaason of not-higher-accuracy-than-81% is that the model dont set higher negative prob. to the word \"not\" and higher positive prob. to the word \"great\" ')\n",
    "print('Other reason can be failure in annotating the reviews and difficulty of annotating to neutral words such as \"goo\" ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
